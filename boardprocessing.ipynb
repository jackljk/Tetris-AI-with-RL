{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_tetris\n",
    "from gym_tetris.actions import MOVEMENT\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_tetris.make('TetrisA-v0')\n",
    "env = JoypadSpace(env, MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# import sb3\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\"CnnPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model.learn(total_timesteps=int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jack/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Custom_Tetris_Env.tetris import TetrisEnv\n",
    "from Custom_Tetris_Env.jp import JoypadSpace\n",
    "from Custom_Tetris_Env.actions import MOVEMENT\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "\n",
    "\n",
    "env = TetrisEnv()\n",
    "env = JoypadSpace(env, MOVEMENT)\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.58e+03 |\n",
      "|    ep_rew_mean      | -20      |\n",
      "|    exploration rate | 0.674    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 650      |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total timesteps  | 34310    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.11e+03 |\n",
      "|    ep_rew_mean      | -20      |\n",
      "|    exploration rate | 0.383    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 628      |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total timesteps  | 64905    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000385 |\n",
      "|    n_updates        | 3726     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.74e+03 |\n",
      "|    ep_rew_mean      | -20      |\n",
      "|    exploration rate | 0.118    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 606      |\n",
      "|    time_elapsed     | 153      |\n",
      "|    total timesteps  | 92860    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000104 |\n",
      "|    n_updates        | 10714    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.42e+03 |\n",
      "|    ep_rew_mean      | -19.9    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 597      |\n",
      "|    time_elapsed     | 198      |\n",
      "|    total timesteps  | 118799   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.07e-05 |\n",
      "|    n_updates        | 17199    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.21e+03 |\n",
      "|    ep_rew_mean      | -19.9    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 589      |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total timesteps  | 144255   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.35e-05 |\n",
      "|    n_updates        | 23563    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.82e+03 |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 586      |\n",
      "|    time_elapsed     | 279      |\n",
      "|    total timesteps  | 163789   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.78e-05 |\n",
      "|    n_updates        | 28447    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.9e+03  |\n",
      "|    ep_rew_mean      | -19.8    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 581      |\n",
      "|    time_elapsed     | 332      |\n",
      "|    total timesteps  | 193169   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.43e-05 |\n",
      "|    n_updates        | 35792    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "model.learn(total_timesteps=int(1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to file\n",
    "model.save(\"save_models/ppo_tetris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_dict = {'Tu': 0, 'Tr': 1, 'Td': 2, 'Tl': 3, 'Jl': 4, 'Ju': 5, 'Jr': 6, 'Jd': 7, 'Zh': 8, 'Zv': 9, 'O': 10, 'Sh': 11, 'Sv': 12, 'Lr': 13, 'Ld': 14, 'Ll': 15, 'Lu': 16, 'Iv': 17, 'Ih': 18}\n",
    "\n",
    "def process_board(board):\n",
    "    # Returns board, holes and boundaries\n",
    "    board = np.where(board == 239,0,1)\n",
    "    holes = np.zeros(board.shape[1], dtype=int)\n",
    "    boundaries = np.zeros(board.shape[1], dtype=int)\n",
    "\n",
    "    for col in range(board.shape[1]):\n",
    "        column_data = board[:, col]\n",
    "        first_block_idx = np.where(column_data == 1)[0]\n",
    "        if first_block_idx.size > 0:\n",
    "            first_block_idx = first_block_idx[0]\n",
    "            boundaries[col] = board.shape[0] - first_block_idx\n",
    "            holes[col] = np.count_nonzero(column_data[first_block_idx:] == 0)\n",
    "    \n",
    "    return board,holes,boundaries\n",
    "\n",
    "def concat_data(holes,boundaries,cur_piece,cleared_lines):\n",
    "    combined_array = np.concatenate((holes, boundaries))\n",
    "    final_array = np.append(combined_array, [cur_piece, cleared_lines])\n",
    "    return final_array\n",
    "\n",
    "done = True\n",
    "for step in range(5000):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    cur_piece = piece_dict[info[\"current_piece\"]]\n",
    "    cleared_lines = info[\"number_of_lines\"]\n",
    "    board, holes, boundaries = process_board(env.board)\n",
    "    state_data = concat_data(holes,boundaries,cur_piece,cleared_lines)\n",
    "    state_data = state_data.copy()\n",
    "    state_data = torch.tensor(state_data, dtype=torch.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 3., 2., 2., 0., 1., 2., 0., 0., 7., 6., 6., 6., 6., 3., 4., 4.,\n",
       "        0., 0., 3., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
